Project Description

The AI-Powered Code Reviewer & Quality Assistant is a production-grade system that automatically analyzes Python source code, evaluates documentation quality, computes code complexity metrics, and assists developers in generating PEP-257 compliant docstrings using Large Language Models (LLMs).

The system combines static analysis, AI-assisted documentation, validation, reporting, and an interactive Streamlit dashboard, behaving like a mini IDE ğŸ§  + CI quality gate ğŸš¦.

All changes are safe, reversible, and user-controlled, making it ideal for certification, academic evaluation, and real-world engineering.

âœ¨ Key Features

ğŸ” AST-based Python source code parsing

ğŸ§  AI-powered docstring generation (preview only)

ğŸ“˜ Supports Google, NumPy, and reST docstring styles

âœ… PEP-257 docstring validation

ğŸ“ Cyclomatic complexity & maintainability metrics

ğŸ“Š Accurate documentation coverage reporting

ğŸ”„ Before vs After diff preview

ğŸ–¥ï¸ Interactive Streamlit review dashboard

ğŸ” Safe Accept / Reject workflow

ğŸ§ª Pytest-based automated test suite

ğŸ“ JSON reports for CI/CD & certification use

ğŸ§  Techniques Used
ğŸ“– Natural Language Processing (NLP)

Semantic understanding of function behavior

Argument and return value interpretation

Context-aware documentation generation

ğŸ¯ Prompt Engineering

Strict JSON-only LLM responses

Imperative-style summaries (PEP-257 compliant)

No hallucinated exceptions or metadata

âœï¸ LLM-Based Text Generation

Generates semantic content only

Formatting handled deterministically in code

Fully configurable LLM backend

ğŸ› ï¸ Tech Stack
ğŸ’» Programming Language

Python 3.9+

ğŸ“š Libraries & Frameworks

ast â€“ Python Abstract Syntax Tree parsing

streamlit â€“ Interactive dashboard

pytest â€“ Automated testing

pytest-json-report â€“ CI-ready test reports

pydocstyle â€“ PEP-257 validation

radon â€“ Complexity & maintainability metrics

python-dotenv â€“ Environment variable management

ğŸ¤– AI / LLM Technologies

Transformer-based Large Language Models

LangChain orchestration

Groq-powered LLM backend (default)

ğŸ§¬ LLM Details

Uses transformer-based LLMs

Default model: llama-3.1-8b-instant

ğŸ” LLM backend is fully configurable

Can be replaced with OpenAI, local LLMs, or other providers

ğŸš« LLM generates content only â€” never modifies code directly

ğŸ“‚ Project Structure
AI_POWERED_CHATBOT/
â”‚
â”œâ”€â”€ ai_powered/
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â””â”€â”€ commands.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ docstring_engine/
â”‚   â”‚   â”‚   â”œâ”€â”€ generator.py
â”‚   â”‚   â”‚   â””â”€â”€ llm_integration.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ parser/
â”‚   â”‚   â”‚   â””â”€â”€ python_parser.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ reporter/
â”‚   â”‚   â”‚   â””â”€â”€ coverage_reporter.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ review_engine/
â”‚   â”‚   â”‚   â””â”€â”€ ai_review.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ validator/
â”‚   â”‚       â””â”€â”€ validator.py
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ sample_a.py
â”‚   â””â”€â”€ sample_b.py
â”‚
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ reports/
â”‚   â””â”€â”€ review_logs.json
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â”œâ”€â”€ test_generator.py
â”‚   â”œâ”€â”€ test_validator.py
â”‚   â””â”€â”€ test_llm_integration.py
â”‚
â”œâ”€â”€ main_app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md

âš™ï¸ Installation Steps
1ï¸âƒ£ Clone the Repository
git clone your_github_link
cd AI_POWERED_CHATBOT

2ï¸âƒ£ Create Virtual Environment
python -m venv ai_powered

3ï¸âƒ£ Activate Virtual Environment (Windows)
Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned
.\ai_powered\Scripts\Activate.ps1

4ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

5ï¸âƒ£ Set Environment Variables

Create a .env file:

GROQ_API_KEY=your_api_key_here

â–¶ï¸ How to Run the Project Locally
ğŸ–¥ï¸ Run Streamlit Dashboard
streamlit run main_app.py

ğŸ§ª Run Tests
pytest --json-report --json-report-file=storage/reports/pytest_results.json


ğŸ“œ License

This project is licensed under the MIT License.
You are free to use, modify, and distribute this software with attribution.